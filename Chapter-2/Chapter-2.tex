%------------------------------%
\chapter{Introduction}
\label{chap:intro}
%------------------------------%

\begin{tcolorbox}[width=\columnwidth,
    tikznode boxed title,
    enhanced,
    interior style={white},
    boxsep=3pt,left=3pt,right=3pt,bottom=2pt,
    width=\columnwidth,
    boxrule=1pt,
    attach boxed title to top center= {yshift=-\tcboxedtitleheight/2},
    colbacktitle=white,coltitle=black,
    boxed title style={size=normal,colframe=white,boxrule=0pt},
    title={My Thesis},
    ]%%
  \fit{Software developers can engineer personal agents that deliver to its stakeholders an ethical and privacy-respecting social experience by modeling and reasoning about social norms, social contexts, and value preferences of the stakeholders.}
\end{tcolorbox}

\nsa{Social Computing: Social Informatics to Social Intelligence \cite{wang2007social}}

Privacy encompasses both technical and technical aspects. But much of the
literature in privacy has focused on these aspects as two
different goals.  Some research aims to design secured systems with the help of
cryptographic protection. Other research aims to protect personal information
by facilitating informed choice options to an individual and assume that
policies and regulations are enforceable. This research tackles
the science of privacy from a sociotechnical viewpoint that bridges the 
two goals.

Human interactions in a society are driven not merely by personal needs
and expectations as Chapter~\ref{chap:arnor} explains. 
Others around us and their expectations play a
prominent part on the way we act and interact. A personal agent acts and
interacts on behalf of its human user. 

A \emph{socially intelligent personal agent} (SIPA) adheres to
\emph{social expectations} of its \emph{primary} and \emph{secondary} 
\emph{stakeholders} (defined later in
Chapter~\ref{chap:arnor}), adapts according to the circumstances or
\emph{social context} \citep{Dey-2001-Context}, acts on behalf of its
user (primary stakeholder), and provides a pleasing social
experience to all of its stakeholders as opposed to an individual
experience to its user.

%This dissertation addresses the research question of how can we
%engineer social intelligence in personal agents to deliver a
%privacy-preserving social experience. 

The key objectives of this research are: (1) to engineer personal agents
such that they deliver a pleasant social experience, 
and yet preserve their stakeholders' privacy, and (2) to make
engineering of such personal agents efficient and effective for
developers. We recognize social norms, social context, and value preferences 
of stakeholders as important factors that influence the working of 
such personal agents.

%A distinguishing feature of envisoned agents is that these agents 

%To achieve these objectives, in my research I apply techniques from
%conceptual modeling, argumentation, and crowdsourcing. I have emphasized
%rigorous evaluation of the proposed technique by empirical research, and
%have conducted several studies including human subject studies and
%simulation experiments.

\section{Preliminaries}

We now provide some necessary background on values, privacy, and social norms in
multiagent systems.

\subsection{Values}

The concept of \fsl{value} has two main connotations: one is about the economic worth of something and the other, more broadly, refers to what people consider important in their lives \citep{Friedman-2008-value-sensitive-design}. 
We adopt the later connotation in this work.

Values are mostly universal across human societies, as stated by \citet{schwartz2012overview} and \citet{rokeach1973nature}. 
The values in Schwartz's \citeyr{schwartz2012overview} work are broad motivational goals, such as stimulation, achievement, security, and benevolence. 
\citet{rokeach1973nature} proposes two types of values---\fsl{terminal} and \fsl{instrumental}. 
Terminal values, such as security, freedom, happiness, and recognition, refer to defined-end states of existence. 
Instrumental values refer to modes of behavior or means to promote the terminal values. 
%
Ethicists subsume ethics in the theory of values \citep{Friedman+08:value-sensitive-design}.
% 
We recognize an ability to understand these values as an important aspect in a SIPA for it to deliver an ethical experience.
\citet{Dechesne-AIL13-Norms+Values} define values as ideals worth pursuing. They observe that these ideals could conflict since
they may not be preferred equally by each individual (that is, each SIPA stakeholder, in our work).


\subsection{Privacy}
The concept of privacy encircles several areas. An individual's notion of
privacy is partly based upon the society's notion of privacy and partly
based upon his or her personal experiences \citep{westin1967privacy,
westin2003social}. The attitude toward sharing personal
information varies from one individual to other.
\citet{westin1967privacy} classifies individuals based on their privacy
preferences: \textit{privacy fundamentalists} are the individuals who
are extremely concerned about their privacy and are reluctant to share
personal information; \textit{privacy pragmatists} are concerned about
privacy but less than fundamentalists and they are willing to disclose
personal information when some benefit is expected; and,
\textit{privacy unconcerned} do not consider privacy loss when
disclosing personal information \citep{westin1967privacy}. The
pragmatists are further grouped into \emph{identity-aware} and \emph{profile-aware}
individuals \citep{spiekermann2009enggprivacy}. Identity-aware
individuals are those who are more concerned about revealing
identifying information such as e-mail or physical address rather than
revealing their interests. Profile-aware individuals worry more about
sharing their hobbies, age, interests, or preferences.

Privacy has been a topic of debate and has no globally
agreed-upon definition \citep{smith2007privacy}. Theorists from the
areas of law, philosophy, sociology, and computer science have 
defined privacy in their respective perspectives.
The importance of privacy and what it brings to an individual is also
debated. Although there is no single definition, researchers 
acknowledge the idea of privacy, and view it as a
collection of concepts instead of one specific concept
\citep{smith2007privacy}. 

Privacy has been considered a right, and is protected by regulations. 
\citet{Prosser-60:Privacy} discusses the right of privacy from a legal perspective. 
\citet{solove-2006-taxonomy} provides a taxonomy of activities that can violate privacy. 
The purpose of Solove's taxonomy is to aid the development of privacy laws, in protecting the right of privacy.
\citet{Spiekermann-2012-Challenges+PrivacyDesign} lists the challenges of Privacy by Design, a proposed solution to the regulation of privacy, including the differentiation between privacy and security and detailed methods to incorporate privacy into system design. 
\citet{westin2003social} dissects the values of privacy in modern societies from political, sociocultural, and personal dimensions.
He defines privacy as a claim of an individual and, when recognized by law and social convention, a right, to determine the revelation of his or her information.


Privacy is inherently a human value \citep{spiekermann2009enggprivacy,smith2007privacy}. 
In regards to ethics, privacy is considered as an ethical value \citep{Langheinrich-01:privacy,Taylor-2002-PrivacyAutonomy}. 


\paragraph*{Privacy as an Ethical Value.}
In this research, we understand privacy as a value with an ethical import \citep{Langheinrich-01:privacy,Taylor-2002-PrivacyAutonomy}. 
We adopt the nuanced notions (specifically intrusion, appropriation, and disclosure) of privacy as defined in Solove's taxonomy \citep{solove-2006-taxonomy}. 


\subsection{Social Norms and Multiagent Systems}

The concept ``norm'' is used in different disciplines and thus has
variant notions such as social expectations, legal laws, and linguistic
imperatives \citep{Boella2009NormativeSystems}. We adopt the concept of
social norms, which describe interactions between principals in terms of
what they ought to be, or reactions to behaviors, including attempts to
apply sanctions. Thus, social norms regulate the interactions of the
principals involved. Norms and normative systems are gaining increasing
interest in the computer science community. \citet{Meyer+Wieringa-93}
define normative systems as ``systems in the behavior of which norms
play a role and which need normative concepts in order to be described
or specified.'' Normative multiagent systems as a research area can be
defined as the intersection of normative systems and multiagent systems.
Norms govern much of our social lives, and therefore are considered as a
key element of artificial agents that are expected to behave comparably
to humans \citep{boella2006normative}. 

We adopt Singh's
\shortcite{Singh-2013-Norms} representation of social norms in which norms
are classified as five types: commitment, authorization, prohibition,
sanction and power.
We consider two main norm types in this work: commitment and prohibition. 
A commitment norm means its subject is committed to its object to bring about a consequent if an antecedent holds. 
For instance, \fsl{Frank} (subject), a high school student is \fsl{committed} (norm) to \fsl{Grace} (object), his mother, that he \fsl{will keep Grace updated about his location} (consequent) when he is \fsl{away from home} (antecedent).
A prohibition norm means its subject is forbidden by its object to bring about a consequent if an antecedent holds. 
For instance, \fsl{Frank} (subject), is \fsl{prohibited} (norm) by Heidi (object), his class teacher, from \fsl{answering phone calls} (consequent) when he is \fsl{in a classroom} (antecedent).


\section{Motivating Example}
\label{sec:intro-example}

\begin{example}
\label{ex:intro-ringer-meeting} 
Consider a ringer manager as a SIPA. The ringer manager installed on 
Alice's phone decides appropriate ringer modes (loud, silent, or 
vibrate) for incoming calls. Alice, the phone owner is the 
primary stakeholder of the SIPA. Bob, Alice's friend who 
calls Alice often, and Charlie and Dave, Alice's coworkers, who are in 
her vicinity, are some of the secondary stakeholders. Further, the ringer 
manager's capabilities influencing its social experience include
\begin{enumerate*}[label=(\arabic*)]
\item allowing Alice to be tele-reachable, 
\item notifying the caller if Alice is not reachable,
\item enabling Alice to work uninterrupted, and
\item not annoying Alice's neighbors.
\end{enumerate*}
\end{example}

Suppose that Bob calls Alice when she is in an important meeting with
Charlie and Dave. Alice is \emph{committed} (a social norm)
to answering Bob's phone calls. Another \emph{commitment} is to keep
one's phone silent during important meetings. Alice's SIPA,
understanding the norms and knowing that Bob's calls to Alice are
generally casual, puts Alice's phone on silent for Bob's call and
notifies Bob that Alice is in a meeting; later when Alice's meeting
ends, Alice's SIPA reminds her to call Bob.

Should Alice's phone rings loudly during the meeting, privacy
implications may follow
\citep{Murukannaiah-IC16-Engineering,solove-2006-taxonomy}. A loud ring
\emph{intrudes} upon Alice's and other meeting attendees' privacy in
that call violates the meeting attendees' reasonable expectation to be
left alone. Further, it is likely that meeting attendees frown at Alice
(\emph{disapprobation}). If Alice answers the call, those overhearing
Alice and Bob's conversation can gain knowledge about her and her
interlocutor (\emph{information leak}). If Bob's call were urgent, Bob's
SIPA could communicate the urgency to Alice's SIPA, and Alice's SIPA
could deliver a different social experience, e.g., set the phone on vibrate
to notify Alice of the urgent call and yet not annoy other meeting attendees.
Should Alice's phone stay silent for Bob's urgent call, it may affect
Alice's and Bob's social relationship.

In the examples above, ringer manager SIPA makes nontrivial decisions
influencing social experience of its stakeholders. Existing software engineering methods
\citep{Bresciani-JAAMAS04-Tropos,Winikoff-2004-DIA,Murukannaiah-AAMAS14-Xipho}
are good starting point to engineer personal agents, however these
methods do not guide developers with systematic steps to represent and
reason about such scenarios, and thus fall short in supporting agents
that adapt to evolving social contexts at runtime.

Social norms inform personal agents about a set of reasonable actions in a social
context \citep{vanRiemsdijk-AAMAS15-SociallyAdaptive}. Norm compliance in
a social context is achieved either by (1) conveyance of norms, where
SIPAs are made aware of norms by direct communication, or (2) via
(positive and negative) sanctions, where personal agents learn norms in the form
of which actions are appropriate in a context
\citep{Andrighetto-2013-PunishVoice}. 

Under certain circumstances, we (as humans) may deviate from norms. When
we deviate, we may offer an explanation typically revealing the context
of the deviation. Revealing context may lessen the burden of deviation,
and may help us avert sanctions resulting from the deviation. Deviations
from norms often hint toward a different norm that is contextually
more relevant. For instance, if Alice reveals to meeting attendees' that the
call was from a sick friend who needs urgent care, the meeting
attendees' (1) may not frown on Alice, and (2) may learn that although
it is not appropriate to answer calls during meetings as it intrudes
upon attendees' privacy, answering an emergency call is acceptable as it
could ensure someone's well being or safety. An ability to reason about
the deviation context, and an understanding of the values promoted or demoted by different
actions could assist SIPAs in providing a pleasing social experience to
its stakeholders.

We recognize three key challenges. One, understanding what constitutes a
social experience, and how SIPA's actions influence the social
experience and privacy of its stakeholders? When SIPAs satisfy or violate
norms, they might share certain contextual information related to
satisfaction or violation. Social experience depends largely on how
SIPAs' stakeholders perceive shared information. Two, how and what
contextual information should a SIPA disclose? When norms conflict,
SIPAs must perform actions that promote richer social experience. Three,
how can we develop decision support to recommend actions?

\section{Research Questions}
\label{sec:intro-questions}

Based on the aforementioned challenges and nuances illustrated by the above example, we seek 
to investigate the following research questions: 

\begin{description}

\item[RQ\fsub{1} Social Intelligence.] How can modeling social intelligence in a SIPA help deliver a social experience and respect its stakeholders' privacy?

\item[RQ\fsub{2} Context.] How can SIPAs share and adapt to deviation contexts, and learn contextually relevant norms? 

\item[RQ\fsub{3} Values.] How can a SIPA reason about values promoted or demoted by its actions and understand preferences among these values?
% Does an ability to reason about values promoted or demoted by actions and an understanding of preferences among these values help a SIPA deliver a value-driven social experience to all its stakeholders?

\end{description}

\section{Contributions}
\label{sec:intro-contributions}

To address the research questions of modeling social intelligence and
enabling ability to reason about deviation contexts and values, we
develop (1) \frameworkA, a software engineering method; (2) \frameworkB, a context
reasoning approach; and (3) \frameworkAinur, a value-based decision-making framework.

\subsection[Modeling Social Intelligence via Norms]{\frameworkA: Modeling Social Intelligence via Norms}

% It is based on a paper ``Arnor: Modeling Social Intelligence via Norms to Engineer Privacy-Aware Personal Agents'' 
This work appears in Proceedings of the \emph{16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2017} as a paper ``Arnor: Modeling Social Intelligence via Norms to Engineer Privacy-Aware Personal Agents'' \citep{Ajmeri-AAMAS17-Arnor}.

To address the research question of modeling social intelligence, we
develop \frameworkA (Chapter~\ref{chap:arnor}), a systematic 
software engineering method. It facilitates developers to model stakeholders' actions
and expectations, and how these influence each other. \frameworkA
employs Singh's \shortcite{Singh-2013-Norms} model of (social) norms to
capture social requirements, and incorporates argumentation constructs
\citep{BenchCapon-2007-Argumentation+AI} for sharing decision rationale.
Since, testing a SIPA's adaptability in all possible social contexts is
logistically challenging and time consuming, \frameworkA also
incorporates a SIPA simulation testbed. We rigorously evaluate
\frameworkA via a developer study and a set of simulation experiments on
the simulation testbed. 

%The novelty of the research is 
%that in spirit, \frameworkA is a hybrid method that addresses the 
%problem of engineering SIPA's by combining both top-down (by modeling) 
%and bottom-up (via experience or social learning) styles.

% \paragraph*{Developer Study} 
We hypothesize that the developers who follow \frameworkA (1) produce
better models, (2) expend less time during application development, (3) feel it is easier to develop a
SIPA, and (4) expend less effort, than those who follow Xipho \cite{Murukannaiah-AAMAS14-Xipho}, an existing software engineering methodology geared toward engineering personal agents. We find
that developers using \frameworkA spend less time and effort, and
overall feel it is easier to engineer a SIPA using \frameworkA. No
significant difference is found in the model quality.

% \paragraph*{Simulation Experiment}
We hypothesize that SIPAs developed using \frameworkA (1) have better
adaptability features, and (2) provide richer social experience, than
SIPAs developed using Xipho. We measure social experience via norm
compliance and sanction proportion measures. We find that SIPAs
engineered using \frameworkA have greater adaptability correctness,
similar norm compliance, and are prone to lesser sanctions.

% \subsection[Enhancing Social Experience via Context Sharing]{\frameworkB: Enhancing Social Experience via Context Sharing}
\subsection[Understanding Social Context]{\frameworkB: Understanding and Reasoning about Social Context}

This work appears in Proceedings of the \emph{27th International Joint Conference on Artificial Intelligence (IJCAI), 2018}
as a paper ``Robust Norm Emergence by Revealing and Reasoning about Context: Socially Intelligent Agents for Enhancing Privacy'' \citep{IJCAI-18:Poros}.

% As SIPAs act and interact, they need to be aware of their stakeholders'
% contexts, and how each stakeholder perceive their actions. To address
% the research question of sharing and reasoning about deviation contexts,
% we develop \frameworkB. \frameworkB is a framework which enables SIPAs
% to share deviation contexts with other agents, and provides SIPAs an
% ability to reason about contexts shared by other agents. This ability to
% share and reason about shared contexts assist SIPAs in inferring
% contextually relevant norms, and thus help SIPAs act in a way that
% provides a pleasing social experience to their stakeholders. We
% evaluated \frameworkB by social simulation experiments.

% \paragraph*{Simulation Experiment}
% We hypothesize that \frameworkB SIPAs that share and reason about
% context learn contextually relevant norms, and thus provide a greater
% social experience than those that don't reason about context. We measure
% social experience through happiness and experience payoff metrics
% (defined in Chapter~\ref{chap:poros}), and find that \frameworkB
% SIPAs provide higher happiness, and experience payoff.

Norms describe the social architecture of a society
and govern the interactions of its member agents.
It may be appropriate for an agent to deviate from
a norm; the deviation being indicative of a specialized
norm applying under a specific context. Existing
approaches for norm emergence assume simplified
interactions wherein deviations are negatively
sanctioned. To address the research question of understanding
social context, we develop \frameworkB (Chapter~\ref{chap:poros}), an approach for
building SIPAs that carry out enriched interactions
where deviating SIPAs share selected elements of their 
context, and other SIPAs respond appropriately to the deviations in light of the received information.

% \paragraph*{Simulation Experiment}
We investigate via simulation the benefits
of such enriched interactions. We find
that as a result (1) the norms are learned better with
fewer sanctions, indicating improved social cohesion;
and (2) the agents are better able to satisfy
their individual goals. These results are robust under
societies of varying sizes and characteristics reflecting
pragmatic, considerate, and selfish agents.

% \subsection[Incorporating Values and Ethics]{\frameworkAinur: Incorporating Values and Ethics}
\subsection[Reasoning about Values and Ethics]{\frameworkAinur: Reasoning about Values and Ethics}

Parts of this work appears in \emph{IEEE Internet Computing} magazine as a column ``Designing Ethical Personal Agents''
\citep{Ajmeri-IC18-Ethical}
and in Proceedings of the \emph{5th Annual Symposium and Bootcamp on Hot Topics in the Science of Security (HotSoS)} as a poster paper ``Ethics, Values, and Personal Agents''
\citep{HotSoS-18:ethics}.

Privacy, values, and ethics are closely intertwined. 
Preserving privacy presumes understanding of human values 
and acting ethically. 
% 
If norms require agents to perform or not perform certain actions,
values provide a reason to or not to pursue those actions
\citep{Dechesne-AIL13-Norms+Values}. Each action a \frameworkB SIPA
executes potentially promotes or demotes one or more \fsl{values}.
For instance, a callee's action of
answering an urgent phone call during a meeting may promote \fsl{safety}
of the caller, but demote \fsl{privacy} of the meeting attendees. 
Being aware of these values and having an ability to reason about them 
helps a SIPA select ethical actions and yield pleasant experience.

To address the research question of reasoning about values, we 
propose \frameworkAinur, a framework to design such ethical 
SIPAs. We incorporate a multicriteria decision-making method 
in \frameworkAinur to aggregate value preferences of stakeholders and 
select an ethically appropriate action. 

% \paragraph*{Simulation Experiment}
 
We empirically evaluate \frameworkAinur via multiple simulation 
experiments. We find that agents developed using \frameworkAinur produce ethical actions that 
% exhibit the Rawlsian property of fairness and 
yield a pleasant social experience to its stakeholders.

\section{Organization}
Chapter~\ref{chap:arnor} details \frameworkA, and discusses its evaluation. 
Chapter~\ref{chap:poros} describes \frameworkB and its 
evaluation via simulation experiments. 
Chapter~\ref{chap:ainur} describes \frameworkAinur, and its empirical evaluation.  
Chapter~\ref{chap:conclusions} concludes with important future directions.