%------------------------------%
\chapter{Introduction}
\label{chap:intro}
%------------------------------%

Privacy encompasses both technical and technical aspects. But the
literature in privacy research has focused on these aspects as two
different goals.  One aims to design secured systems with the help of
cryptographic protection. The other aims to protect personal information
by facilitating informed choice options to an individual and assume that
policies and regulations are enforceable. This research tackles
the science of privacy from a sociotechnical viewpoint that bridges the 
two goals.

Human interactions in a society are not merely driven by personal needs
and expectations (defined later in Chapter~\ref{sec:arnor-framework}). 
Others around us and their expectations play a
prominent part on the way we act and interact. A personal agent acts and
interacts on behalf of its human user. 

A \emph{socially intelligent personal agent} (SIPA) adheres to
\emph{social expectations} of multiple \emph{stakeholders}---both
\emph{primary} and \emph{secondary} (defined later in
Chapter~\ref{chap:arnor}), adapts according to the circumstances or
\emph{social context} \citep{Dey-2001-Context}, acts on behalf of its
human user (primary stakeholder), and provides a pleasing social
experience to all of its stakeholders as opposed to an individual
experience to its human user.

%This dissertation addresses the research question of how can we
%engineer social intelligence in personal agents to deliver a
%privacy-preserving social experience. 

The key objectives of this research are: (1) to engineer personal agents
such that they deliver a pleasant social experience relative to the
society, and yet preserve their stakeholders' privacy, and (2) to make
engineering of such personal agents efficient and effective for
developers. We recognize social norms and social context as important
factors that influence the working of such personal agents.

%A distinguishing feature of envisoned agents is that these agents 

%To achieve these objectives, in my research I apply techniques from
%conceptual modeling, argumentation, and crowdsourcing. I have emphasized
%rigorous evaluation of the proposed technique by empirical research, and
%have conducted several studies including human subject studies and
%simulation experiments.

\section{Preliminaries}

In this section we provide a background on values, privacy, and social norms in
multiagent systems.

\subsection{Social Norms and Multiagent Systems}

The concept ``norm'' is used in different disciplines and thus has
variant notions such as social expectations, legal laws, and linguistic
imperatives \citep{Boella2009NormativeSystems}. We adopt the concept of
social norms, which describe interactions between principals in terms of
what they ought to be, or reactions to behaviors, including attempts to
apply sanctions. Thus, social norms regulate the interactions of the
principals involved. Norms and normative systems are gaining increasing
interest in the computer science community. \citet{Meyer+Wieringa-93}
define normative systems as ``systems in the behavior of which norms
play a role and which need normative concepts in order to be described
or specified.'' Normative multiagent systems as a research area can be
defined as the intersection of normative systems and multiagent systems.
Norms govern much of our social lives, and therefore are considered as a
key element of artificial agents that are expected to behave comparably
to humans \citep{boella2006normative}. We adopt Singh's
\shortcite{Singh-2013-Norms} representation of social norms in which norms
are classified as five types: commitment, authorization, prohibition,
sanction and power.


\subsection{Values}

The concept of \fsl{value} has two main connotations: one is about economic worth of something and the other, more broadly, refers to what people consider important in their lives \citep{Friedman-2008-value-sensitive-design}. 
We adopt the later connotation in this work.

Values are mostly universal across human societies as stated by \citet{schwartz2012overview} and \citet{rokeach1973nature}. 
The values in Schwartz's \citep{schwartz2012overview} work are broad motivational goals, such as stimulation, achievement, security, and benevolence. 
\citet{rokeach1973nature} proposes two types of values---\fsl{terminal} and \fsl{instrumental}. 
Terminal values, such as security, freedom, happiness, and recognition, refer to defined-end states of existence. 
Instrumental values refer to modes of behavior or means to promote the terminal values. 
%
We recognize an ability to understand these values as an important aspect in a SIPA for it to deliver an ethical experience.
\citet{Dechesne-AIL13-Norms+Values} define values as ideals worth pursuing and observe that these ideals could conflict since
they may not be preferred equally by each individual (that is, each SIPA user, in our work).


\subsection{Privacy}
The concept of privacy encircles several areas. An individual's notion of
privacy is partly based upon the society's notion of privacy and partly
based upon his or her personal experiences \citep{westin1967privacy,
westin2003social}. The general attitude toward sharing personal
information varies from one individual to other.
\citet{westin1967privacy} classifies individuals based on their privacy
preferences: \textit{privacy fundamentalists} are the individuals who
are extremely concerned about their privacy and are reluctant to share
personal information; \textit{privacy pragmatists} are concerned about
privacy but less than fundamentalists and they are willing to disclose
personal information when some benefit is expected; and,
\textit{privacy unconcerned} do not consider privacy loss when
disclosing personal information \citep{westin1967privacy}. The
pragmatists are further grouped into \emph{identity-aware} and \emph{profile-aware}
individuals \citep{spiekermann2009enggprivacy}. Identity-aware
individuals are the ones who are more concerned about revealing
identifying information such as e-mail or physical address rather than
revealing their interests. Profile-aware individuals worry more about
sharing their hobbies, age, interests, or preferences.

Privacy has always been a topic of debate and has no globally
agreed-upon definition \citep{smith2007privacy}. Theorists from the
areas of law, philosophy, sociology, and computer science have all tried
to define privacy in their own perspective.
The importance of privacy and what it brings to an individual is also
debated. Although there is no single definition, researchers 
acknowledge the idea of privacy, and view it as a
collection of concepts instead of one specific concept
\citep{smith2007privacy}. 

Privacy has been considered a right, and is protected by regulations. 
\citet{Prosser-60:Privacy} discusses the right of privacy from a legal perspective. 
\citet{solove-2006-taxonomy} provides a taxonomy of activities that can violate privacy. 
The purpose of this taxonomy is to aid the development of privacy laws, in protecting the right of privacy.
\citet{Spiekermann-2012-Challenges+PrivacyDesign} lists the challenges of Privacy by Design, a proposed solution to the regulation of privacy, including the differentiation between privacy and security and detailed methods to incorporate privacy into system design. 
\citet{westin2003social} dissects the values of privacy in modern societies from political, sociocultural, and personal dimensions.
He defines privacy as a claim of an individual and, when recognized by law and social convention, a right, to determine the revelation of his or her information.


Privacy is inherently a human value \citep{spiekermann2009enggprivacy,smith2007privacy}. 
Ethicists subsume ethics in the theory of values \citep{Friedman+08:value-sensitive-design}, and regard privacy as an ethical value \citep{Langheinrich-01:privacy,Taylor-2002-PrivacyAutonomy}. 


\paragraph*{Privacy as an Ethical Value.}
In this research, we understand privacy as a value with an ethical import \citep{Langheinrich-01:privacy,Taylor-2002-PrivacyAutonomy}. 
We adopt the nuanced notions (specifically intrusion, appropriation, and disclosure) of privacy as defined in Solove's taxonomy \citep{solove-2006-taxonomy}. 


\subsection{Values and Social Norms}
Representing and reasoning about social norms (in context) is essential to producing an ethical SIPA. 
That is, an ethical SIPA acts in compliance with contextually relevant social norms (but it may choose to break some norms intentionally, e.g., when the norms conflict) \citep{Ajmeri-AAMAS17-Arnor}. 
Even in the case of privacy, social norms are the centerpiece of privacy according to Nissenbaum's theory of \emph{contextual integrity} \citep{Nissenbaum-04:integrity,Nissenbaum-11:online}, where privacy violations occur when information flows do not respect contextual norms.

In general terms, social norms describe interactions between a subject and an object in terms of what they ought to be, or as reactions to behaviors, including attempts to apply sanctions. 
We adopt Singh's \citep{Singh-2013-Norms} representation of social norms, in which a norm is directed from a subject to an object and is constructed as a conditional relationship involving an antecedent (which brings an instance of the norm in force) and a consequent (which brings the norm instance to completion). 
A norm generates a new instance each time it applies. 
This representation yields clarity on who is accountable to whom, when, and for what. 
We consider two main norm types in the present study: commitment and prohibition. 
A commitment norm means its subject is committed to its object to bring about a consequent if an antecedent holds, and a prohibition norm means its subject is forbidden by its object to bring about a consequent if an antecedent holds. 
For instance, \fsl{Frank} (subject), a high school student is \fsl{committed} (norm) to \fsl{Grace} (object), his mother, that he \fsl{will keep Grace updated about his location} (consequent) when he is \fsl{away from home} (antecedent).

%\subsection*{Values and Norms}
Whereas norms require agents to perform or not perform certain actions, values provide a reason to pursue or not pursue those actions \citep{Dechesne-AIL13-Norms+Values}. 
Each of a SIPA's actions promotes or demotes certain values. 
For instance, in the phone ringer SIPA example described in \citet{Ajmeri-AAMAS17-Arnor}, a callee's action of answering an urgent phone call during a meeting may promote the value of safety (for the caller), but demote the value of privacy (of the meeting attendees).

Only a few previous works have attempted to relate values with norms.
\citet{Murukannaiah-IC16-Engineering} model actors, context, and social expectations via norms to engineer privacy respecting agents. 
\citet{DaSilvaFigueiredo-COIN13} propose an algorithm to identify conflicts between norms based on values. 
A conflict occurs when
\begin{enumerate*}[label=(\arabic*)]
\item a consequent action of a commitment norm demotes a value, or
\item a consequent action of a prohibition norm promotes a value important to a SIPA user.
\end{enumerate*}
%
\citet{Dechesne-AIL13-Norms+Values} develop a model of norms and culture,
represented by values, to study compliance of norms. They concur that
values are important in deciding whether or not a norm should be
introduced. \citet{kayal13coin} present a model in which norms and 
context are centered on values. Such a model could be employed
to govern a SIPA by identifying value preferences of the SIPA's
users.


\section{Motivational Example}
\begin{example}
\label{ex:intro-ringer-meeting} 
Consider a ringer manager as a SIPA. The ringer manager installed on 
Alice's phone decides appropriate ringer modes (loud, silent, or 
vibrate) for incoming calls. Alice, the phone owner is the 
primary stakeholder of the SIPA. Bob, Alice's friend who 
calls Alice often, and Charlie and Dave, Alice's coworkers, who are in 
her vicinity, are some of the secondary stakeholders. Further, the ringer 
manager's capabilities influencing its social experience include
\begin{enumerate*}[label=(\arabic*)]
\item allowing Alice to be tele-reachable, 
\item notifying the caller if Alice is not reachable,
\item enabling Alice to work uninterrupted, and
\item not annoying Alice's neighbors.
\end{enumerate*}
\end{example}

Suppose that Bob calls Alice when she is in an important meeting with
Charlie and Dave. Alice is \emph{committed} (a social norm)
to answering Bob's phone calls. Another \emph{commitment} is to keep
one's phone silent during important meetings. Alice's SIPA,
understanding the norms and knowing that Bob's calls to Alice are
generally casual, puts Alice's phone on silent for Bob's call and
notifies Bob that Alice is in a meeting; later when Alice's meeting
ends, Alice's SIPA reminds her to call Bob.

Should Alice's phone rings loudly during the meeting, privacy
implications may follow
\citep{Murukannaiah-IC16-Engineering,solove-2006-taxonomy}. A loud ring
\emph{intrudes} upon Alice's and other meeting attendees' privacy in
that call violates the meeting attendees' reasonable expectation to be
left alone. Further, it is likely that meeting attendees frown at Alice
(\emph{disapprobation}). If Alice answers the call, those overhearing
Alice and Bob's conversation can gain knowledge about her and her
interlocutor (\emph{information leak}). If Bob's call were urgent, Bob's
SIPA could communicate the urgency to Alice's SIPA, and Alice's SIPA
could deliver a different social experience, e.g., set phone on vibrate
to notify Alice of urgency and yet not annoy other meeting attendees.
Should Alice's phone stays silent for Bob's urgent call, it may affect
their relationships.

In the examples above, ringer manager SIPA makes nontrivial decisions
influencing social experience of its stakeholders. Existing AOSE methods
\citep{Bresciani-JAAMAS04-Tropos,Winikoff-2004-DIA,Murukannaiah-AAMAS14-Xipho}
are good starting point to engineer personal agents, however these
methods do not guide developers with systematic steps to represent and
reason about such scenarios, and thus fall short in supporting agents
that adapt to evolving social contexts at runtime.

Social norms inform personal agents about a set of reasonable actions in a social
context \citep{vanRiemsdijk-AAMAS15-SociallyAdaptive}. Norm compliance in
a social context is either achieved by (1) conveyance of norms, where
SIPAs are made aware of norms by direct communication, or (2) via
(positive and negative) sanctions, where personal agents learn norms in the form
of which actions are appropriate in a context
\citep{Andrighetto-2013-PunishVoice}. 

Under certain circumstances, we (as humans) may deviate from norms. When
we deviate, we may offer an explanation typically revealing the context
of the deviation. Revealing context may lessen the burden of deviation,
and may help us avert sanction resulting from the deviation. Deviations
from norms often hint toward a different norm that is contextually
relevant. For instance if Alice reveals to meeting attendees' that the
call was from a sick friend who needs urgent care, the meeting
attendees' (1) may not frown on Alice, and (2) may learn that although
it is not appropriate to answer calls during meetings as it intrudes
upon attendees' privacy, answering an emergency call is acceptable as it
could ensure someone's well being or safety. An ability to reason about
deviation context, and an understanding of values promoted or demoted by different
actions could assist SIPAs in providing a pleasing social experience to
its stakeholders.

We recognize three key challenges. One, understanding what constitutes a
social experience, and how SIPA's actions influence the social
experience and privacy of its stakeholders? When SIPAs satisfy or violate
norms, they might share certain contextual information related to
satisfaction or violation. Social experience largely depends on how
SIPAs' stakeholders perceive shared information. Two, how and what
contextual information should a SIPA disclose? When norms conflict,
SIPAs must perform actions that promote richer social experience. Three,
how can we develop decision support to recommend actions?

\section{Research Questions}
\label{sec:intro-questions}

Based on aforementioned challenges and nuances in the example, we seek 
to probe the following research questions: 

\begin{description}

\item[RQ Social Intelligence.] How can modeling social intelligence in a SIPA help deliver a social experience and respect its stakeholders' privacy?

\item[RQ Context.] How can SIPAs share and adapt to deviation contexts, and learn contextually relevant norms? 

\item[RQ Values.] How can a SIPA reason about values promoted or demoted by its actions and understand preferences among these values?
% Does an ability to reason about values promoted or demoted by actions and an understanding of preferences among these values help a SIPA deliver a value-driven social experience to all its stakeholders?

\end{description}

\section{Contributions}
\label{sec:intro-contributions}

To address the research questions of modeling social intelligence and
enabling ability to reason about deviation contexts and values, we
develop (1) \frameworkA, an AOSE method; (2) \frameworkB, a context
reasoning approach; and (3) \frameworkAinur, a value-based decision-making framework.

\subsection[Modeling Social Intelligence via Norms]{\frameworkA: Modeling Social Intelligence via Norms}

To address the research question of modeling social intelligence, we
develop \frameworkA \citep{Ajmeri-AAMAS17-Arnor}, a systematic 
AOSE method. It facilitates developers to model stakeholders' actions
and expectations, and how these influence each other. \frameworkA
employs Singh's \shortcite{Singh-2013-Norms} model of (social) norms to
capture social requirements, and incorporates argumentation constructs
\citep{BenchCapon-2007-Argumentation+AI} for sharing decision rationale.
Since, testing a SIPA's adaptability in all possible social contexts is
logistically challenging and time consuming, \frameworkA also
incorporates a SIPA simulation testbed. We rigorously evaluate
\frameworkA via a developer study and a set of simulation experiments on
the simulation testbed. 

%The novelty of the research is 
%that in spirit, \frameworkA is a hybrid method that addresses the 
%problem of engineering SIPA's by combining both top-down (by modeling) 
%and bottom-up (via experience or social learning) styles.

% \paragraph*{Developer Study} 
We hypothesize that the developers who follow \frameworkA (1) produce
better models, (2) expend less time, (3) feel it is easier to develop a
SIPA, and (4) expend less effort, than those who follow Xipho. We find
that developers using \frameworkA spend less time and effort, and
overall feel it is easier to engineer a SIPA using \frameworkA. No
significant difference is found in the model quality.

% \paragraph*{Simulation Experiment}
We hypothesize that SIPAs developed using \frameworkA (1) have better
adaptability features, and (2) provide richer social experience, than
SIPAs developed using Xipho. We measure social experience via norm
compliance and sanction proportion measures. We find that SIPAs
engineered using \frameworkA have greater adaptability correctness,
similar norm compliance, and are prone to lesser sanctions.

% \subsection[Enhancing Social Experience via Context Sharing]{\frameworkB: Enhancing Social Experience via Context Sharing}
\subsection[Understanding Social Context]{\frameworkB: Understanding and Reasoning about Social Context}

% As SIPAs act and interact, they need to be aware of their stakeholders'
% contexts, and how each stakeholder perceive their actions. To address
% the research question of sharing and reasoning about deviation contexts,
% we develop \frameworkB. \frameworkB is a framework which enables SIPAs
% to share deviation contexts with other agents, and provides SIPAs an
% ability to reason about contexts shared by other agents. This ability to
% share and reason about shared contexts assist SIPAs in inferring
% contextually relevant norms, and thus help SIPAs act in a way that
% provides a pleasing social experience to their stakeholders. We
% evaluated \frameworkB by social simulation experiments.

% \paragraph*{Simulation Experiment}
% We hypothesize that \frameworkB SIPAs that share and reason about
% context learn contextually relevant norms, and thus provide a greater
% social experience than those that don't reason about context. We measure
% social experience through happiness and experience payoff metrics
% (defined in Chapter~\ref{chap:poros}), and find that \frameworkB
% SIPAs provide higher happiness, and experience payoff.

Norms describe the social architecture of a society
and govern the interactions of its member agents.
It may be appropriate for an agent to deviate from
a norm; the deviation being indicative of a specialized
norm applying under a specific context. Existing
approaches for norm emergence assume simplified
interactions wherein deviations are negatively
sanctioned. To address the research question of understanding
social context, we develop \frameworkB, an approach for
building SIPAs that carry out enriched interactions
where deviating SIPAs share selected elements of there 
context, and others respond appropriately.

% \paragraph*{Simulation Experiment}
We investigate via simulation the benefits
of such enriched interactions. We find
that as a result (1) the norms are learned better with
fewer sanctions, indicating improved social cohesion;
and (2) the agents are better able to satisfy
their individual goals. These results are robust under
societies of varying sizes and characteristics reflecting
pragmatic, considerate, and selfish agents.



% \subsection[Incorporating Values and Ethics]{\frameworkAinur: Incorporating Values and Ethics}
\subsection[Reasoning about Values and Ethics]{\frameworkAinur: Reasoning about Values and Ethics}
Privacy, values, and ethics are closely intertwined. 
Preserving privacy presumes understanding of human values 
and acting ethically. 
% 
If norms require agents to perform or not perform certain actions,
values provide a reason to or not to pursue those actions
\citep{Dechesne-AIL13-Norms+Values}. Each action a \frameworkB SIPA
executes, promotes or demotes certain \fsl{values}.
For instance, a callee's action of
answering an urgent phone call during a meeting may promote \fsl{safety}
of the caller, but demote \fsl{privacy} of the meeting attendees'. 
Being aware of these values and an ability to reason about them 
helps a SIPA select ethical actions and yield pleasant experience.

To address the research question of reasoning about values, we 
propose \frameworkAinur, a framework to design such ethical 
SIPAs. We incorporate a multicriteria decision-making method 
in \frameworkAinur to aggregate value preferences of users and 
select an ethically appropriate action. 

% \paragraph*{Simulation Experiment}
 
We empirically evaluate \frameworkAinur via multiple simulation 
experiments. We find that agents developed using \frameworkAinur produce ethical actions that exhibit the
Rawlsian property of fairness and yield a pleasant social experience to its users.

\section{Organization}
Chapter~\ref{chap:arnor} details \frameworkA, and discusses its evaluation. 
Chapter~\ref{chap:poros} describes \frameworkB and its 
evaluation via simulation experiments. 
Chapter~\ref{chap:ainur} describes \frameworkAinur, and its empirical evaluation.  
Chapter~\ref{chap:conclusions} concludes with important future directions.